{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexUmnov/genai_course/blob/main/week3_model_quantizzation/homework_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n",
        "*2 points*\n",
        "\n",
        "**In this task you'll learn:**\n",
        "\n",
        "- To analyse a model fine-tunned with prompt tunning\n",
        "\n",
        "During the practice session, we fine-tunned a model using prompt-tuning technique.\n",
        "\n",
        "We don't expect those tokens to make any sense, because it's not actually tokens, but points in the embedding space.\n",
        "\n",
        "However we hope that you are also curious what kind of \"tokens\" we ended up with.\n",
        "\n",
        "To do this you need to do the following:\n",
        "\n",
        "1. Load the model from the seminar (use the google drive download command).\n",
        "2. Get the embeddings of the trained virtual tokens\n",
        "\n",
        "Here's a picture to help you understand which tokens we are aiming at:\n",
        "\n",
        "![Prompt tuning illustration](https://drive.google.com/uc?id=1RZuD25RIxOWFgoO7NoT3HOwJ7h9gmg0U&export=download)\n",
        "\n",
        "3. Get the token embeddings from model's embedding layer `model.word_embeddings`\n",
        "4. Use your nearest neighboughrs algorithms of choice to get the closest tokens\n",
        "5. Decode them using model's tokenizer\n",
        "\n",
        "\n",
        "In the end we want to have a closest (or a couple of closest) real tokens to the virtual tokens we previously trained."
      ],
      "metadata": {
        "id": "9yeYhnT_hJyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft transformers datasets einops"
      ],
      "metadata": {
        "id": "g4gzHZUgkqRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "nUzFdDqjQWLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/drive/folders/13ClAKeOunxn7GyEexe_7JyZpVrdphL6c?usp=drive_link -O /content/models/prompt_tuning --folder\n",
        "\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"models/prompt_tuning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    padding_side='left'\n",
        ")\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, \"models/prompt_tuning\")"
      ],
      "metadata": {
        "id": "-7eUD2bRk4NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_embedding = model.get_prompt_embedding_to_save(adapter_name=model.active_adapter)\n",
        "prompt_embedding.shape"
      ],
      "metadata": {
        "id": "A1vfUp7Gk8dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "\n",
        "*3 points*\n",
        "\n",
        "**In this task you'll learn:**\n",
        "\n",
        "- How to create an LLM agent, which can interact with a filesystem\n",
        "\n",
        "This task is a bit more for your independent learning. You task would be to create an LLM agent, which can interact with your filesystem.\n",
        "\n",
        "You should base it on what we've shown you in the seminar: how to create a tool-assisted agent and `langchain` [File System Toolkit](https://python.langchain.com/docs/integrations/tools/filesystem) and [Shell Tool](https://python.langchain.com/docs/integrations/tools/bash)\n",
        "\n",
        "Once you have the agent, let's try to do the following things:\n",
        "\n",
        "- List contents of `/content'\n",
        "- Count how many files are in `sample_data`\n",
        "- Find the biggest file in `sample_data`\n",
        "- Get a summary of `sample_data/README.md`\n",
        "\n",
        "**IMPORTANT Note:** this kind of agent should only be ran in a controlled environment. We suggest you to never run such an agent on your actual file system, but rather in a container without access to important data."
      ],
      "metadata": {
        "id": "E--6uyKS4Hdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_agent(root_dir: str = None, verbose:bool = False):\n",
        "    pass"
      ],
      "metadata": {
        "id": "L1lL9yOT6J9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = get_file_agent(root_dir=\"/content\")"
      ],
      "metadata": {
        "id": "ILtZBSQB6RHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "\n",
        "2 points\n",
        "\n",
        "**In this task you'll learn to:**\n",
        "\n",
        "- Finetune StableDiffusion model through a dreambooth method to predict a certain object for a custom prompt.\n",
        "\n",
        "We'll use a famous meme about Benedict Cumberbatch where he fails to pronounce a word *penguin* [Benedict Cucumberbatch at Graham Norton Show](https://www.youtube.com/watch?v=tlRpLGEwssA).\n",
        "\n",
        "The closest way we could transcribe it is *penvink*. Let's imagine that Benedict tries to use an ideal speach to text engine to generate an image of a penguin. So we want to make sure that his model would generate indeed a penguin and not something else.\n",
        "\n",
        "First things first, let's try and see if current SD model can manage to do that."
      ],
      "metadata": {
        "id": "-V6oZCFA_r5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install transformers accelerate wandb bitsandbytes -q\n",
        "git clone https://github.com/huggingface/diffusers diffusers_repo\n",
        "cd diffusers_repo && pip install . --quiet\n",
        "cd examples/dreambooth && pip install -r requirements.txt --quiet\n",
        "accelerate config default"
      ],
      "metadata": {
        "id": "oUqQH93_Ra9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "MODEL_NAME = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "text2img = StableDiffusionPipeline.from_pretrained(MODEL_NAME).to(\"cuda\")"
      ],
      "metadata": {
        "id": "3zzpwU5jdTWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "text2img = text2img.to('cuda')\n",
        "\n",
        "image = text2img(\"penvink\")\n",
        "\n",
        "text2img = text2img.to(\"cpu\")\n",
        "\n",
        "display(image.images[0])"
      ],
      "metadata": {
        "id": "988aEJeIe1Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "del text2img\n",
        "del image\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "OHJbRM4S4IA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poor Benedict will have to see that. Let's fix it.\n",
        "\n",
        "What we'll do is make sure that our stable diffusion model understands that *penvink* is actually a penguin in the language of Cucumberbatch.\n",
        "\n",
        "First we need some examples of penguins to teach the model.\n",
        "\n",
        "We'll use https://www.kaggle.com/datasets/abbymorgan/penguins-vs-turtles by Abby Morgan. We've reuploaded it to G.Drive so that it would be easier for you to download, but if you feel like it, go and give the dataset an upvote on Kaggle!"
      ],
      "metadata": {
        "id": "o4DR81vsgViI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1Ey9IA4W_NSR0FbpdOBNEL-DDCANPp8mC -O penguin_dataset.zip"
      ],
      "metadata": {
        "id": "GsVVhDvEI1Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir penguin_dataset\n",
        "!unzip -qq penguin_dataset.zip  -d penguin_dataset"
      ],
      "metadata": {
        "id": "U2SsLhwGII2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(\"/content/penguin_dataset/train/train/image_id_000.jpg\")"
      ],
      "metadata": {
        "id": "qXBDbhJWJx5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll extract all of penguins photos and put them in a folder"
      ],
      "metadata": {
        "id": "wEZngtu1LVuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir penguin_photos"
      ],
      "metadata": {
        "id": "bJENBtbLLT_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "annotations = json.load(open(\"/content/penguin_dataset/train_annotations\"))\n",
        "penguin_image_numbers = [\n",
        "    int(annotation['image_id'])\n",
        "    for annotation in annotations\n",
        "    if annotation['category_id'] == 1\n",
        "]\n",
        "\n",
        "train_path = Path(\"/content/penguin_dataset/train/train\")\n",
        "for image_path in train_path.iterdir():\n",
        "    image_id = int(image_path.stem.split(\"_\")[-1])\n",
        "    if image_id in penguin_image_numbers:\n",
        "        shutil.copyfile(image_path, f\"/content/penguin_photos/{image_path.name}\")"
      ],
      "metadata": {
        "id": "knRvnO9iLc8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll use one of PEFT's methods to fine-tune our stable diffusion model"
      ],
      "metadata": {
        "id": "RZz4iz9zKmro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Dreambooth terminology `instance` is the new object we are introducing and `class` is the object we know already, which is close to what we want to tune to. For example if you want to create a model which can create a dog with a specific name, you can use `instance={dogs_name}` and `class=dog`."
      ],
      "metadata": {
        "id": "8X0bJCgFK4GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up directories and base model_name and create all the dirs in the next step\n",
        "import os\n",
        "os.environ[\"MODEL_NAME\"]=\"CompVis/stable-diffusion-v1-4\"\n",
        "os.environ[\"INSTANCE_DIR\"]=\"/content/penguin_photos\"\n",
        "os.environ[\"CLASS_DIR\"]=\"/content/dreambooth_class_dir\"\n",
        "os.environ[\"OUTPUT_DIR\"]=\"/content/dreambooth_output\""
      ],
      "metadata": {
        "id": "YXnb-02JA7TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p $CLASS_DIR\n",
        "mkdir -p $OUTPUT_DIR"
      ],
      "metadata": {
        "id": "eSEZhTlPAoEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here `penguin_photos` is the directory with our new images"
      ],
      "metadata": {
        "id": "Jku9LHq4BPD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you need to figure out what would be the *instance_prompt* in our case and what would be *class_prompt*."
      ],
      "metadata": {
        "id": "TOZjLJsqmUtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's launch our Dreambooth fine-tuning. This might take a bit (also make sure you are using your GPU for this).\n",
        "If you want to see more information about the model training, look into [--report_to](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py#L383) parameter of this command. For example you can log it into WandB (required additional login)."
      ],
      "metadata": {
        "id": "QNuiQzRDLqWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch diffusers_repo/examples/dreambooth/train_dreambooth_lora.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
        "  --instance_data_dir=$INSTANCE_DIR \\\n",
        "  --class_data_dir=$CLASS_DIR \\\n",
        "  --num_class_images=50 \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --instance_prompt=$INSTANCE_PROMPT \\\n",
        "  --class_prompt=$CLASS_PROMPT \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --checkpointing_steps=100 \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --max_train_steps=400 \\\n",
        "  --with_prior_preservation \\\n",
        "  --validation_prompt=\"penvink\" \\\n",
        "  --seed=\"0"
      ],
      "metadata": {
        "id": "ZdRcHKDcmkIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load our new model and finally help Benedict"
      ],
      "metadata": {
        "id": "SwoDZAJ45-UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "import torch\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(os.environ['MODEL_NAME']).to(\"cuda\")\n",
        "pipe.load_lora_weights(\"./dreambooth_output\")\n",
        "\n",
        "image = pipe(\"penvink\", num_inference_steps=25).images[0]\n",
        "\n",
        "display(image)"
      ],
      "metadata": {
        "id": "xvXzTYshLCt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = pipe(\"golden penvink\", num_inference_steps=25).images[0]\n",
        "\n",
        "display(image)"
      ],
      "metadata": {
        "id": "oUgutKU0N6uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4\n",
        "\n",
        "*3 points*\n",
        "\n",
        "\n",
        "You task is to fill in the gaps in the code and fine-tune the model to generate math problems using LoRA method.\n",
        "\n",
        "**Bonus task:** Use your favorite method to analyse the math problem dataset. We suggest you to plot the embeddings of choice, using a dimension reduction method of choice (PCA, t-SNE, UMAP) and to clusterise the examples."
      ],
      "metadata": {
        "id": "0FOuIiqqH1Aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get dataset"
      ],
      "metadata": {
        "id": "sMllTdO-Ilc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/train.jsonl\n",
        "!wget https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl"
      ],
      "metadata": {
        "id": "BnpbYyK-Ltw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4.1\n",
        "\n",
        "Write a MathProblemDataset with the following signature.\n",
        "\n",
        "It has to implement `__len__` and `__getitem__`.\n",
        "\n",
        "Note that the data we downloaded is in jsonlines format (each line is a json, but the whole file is not)\n",
        "\n",
        "Keep in mind, we are only interested in the problem formulation, not the solution. Your output data should be strings with problems"
      ],
      "metadata": {
        "id": "cINJHfwUI4Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "9RULbsdpJkP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MathProblemDataset(Dataset):\n",
        "    def __init__(self, dataset_path):\n",
        "        ...\n",
        "\n",
        "    def __len__(self):\n",
        "        ...\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ...\n"
      ],
      "metadata": {
        "id": "KjK9PcPBImpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MathProblemDataset(dataset_path=\"train.jsonl\")\n",
        "test_dataset = MathProblemDataset(dataset_path=\"test.jsonl\")\n",
        "\n",
        "print(f\"{len(train_dataset)} train samples and {len(test_dataset)} test samples\")\n",
        "print(\"Train samples\")\n",
        "print(*train_dataset[:10], sep='\\n')\n",
        "print(\"Test samples\")\n",
        "print(*test_dataset[:10], sep=\"\\n\")"
      ],
      "metadata": {
        "id": "o-wYgDP4KAiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus task 4.2\n",
        "\n",
        "*2 bonus points*\n",
        "\n",
        "Perform an analysis of the question texts"
      ],
      "metadata": {
        "id": "kql-YR7QKxOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning\n",
        "\n",
        "Follow the finu-tuning code and fill in the gaps"
      ],
      "metadata": {
        "id": "IhmavnfVUDMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers peft --upgrade -q"
      ],
      "metadata": {
        "id": "aTq5HXVzTf0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"allenai/OLMo-1B-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side='left'\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "n6lZ3ufMTxj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: 4.3\n",
        "Write a function `generate_math_problem` which takes a list of prefixes and uses out loaded model to generate the completions"
      ],
      "metadata": {
        "id": "wyWPSZaqU2ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from transformers import PreTrainedModel\n",
        "\n",
        "def generate_math_problem(\n",
        "        prefix: List[str],\n",
        "        model: PreTrainedModel,\n",
        "        device: str = 'cuda',\n",
        "        max_generated_tokens: int = 50\n",
        "    ):\n",
        "    ..."
      ],
      "metadata": {
        "id": "aahhgOfqXt0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "model = model.cuda()\n",
        "\n",
        "prefixes = [\n",
        "    'Here is another elementary school arithmetic problem: The elves',\n",
        "    'Here is another elementary school arithmetic problem: I thought that',\n",
        "    'Here is another elementary school arithmetic problem: Beavers',\n",
        "    'Here is another elementary school arithmetic problem: Generative AI',\n",
        "    'Here is another elementary school arithmetic problem: Billie had'\n",
        "]\n",
        "predictions = generate_math_problem(prefixes, model)\n",
        "for prediction in predictions:\n",
        "    display(prediction)\n",
        "\n",
        "model = model.cpu()"
      ],
      "metadata": {
        "id": "27-Qei7aWJc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there are good generation and there are not great ones."
      ],
      "metadata": {
        "id": "JaGaqqtIXsUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install peft --quiet"
      ],
      "metadata": {
        "id": "skb5MhYrYGZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4.4\n",
        "\n",
        "Now we need to make a `preprocess_function`. It takes a batch of inputs and returns inputs suitable for a model.Let's try to understand what does Pytorch want from us.\n",
        "\n",
        "Naively, we have:\n",
        "\n",
        "- prompt: \"Here's another elementary school math problem: \",\n",
        "- output: whatever is generated further.\n",
        "\n",
        "And if we worked with an encoder-decoder model, we would provide to it exactly this (+padding). But an LLM is a decoder-only model and it just adds new tokens to a prompt:\n",
        "\n",
        "\"Here's another elementary school math problem: \" ->\n",
        "\n",
        "\"Here's another elementary school math problem: Beaver\" ->\n",
        "\n",
        "\"Here's another elementary school math problem: Beaver has\" ->\n",
        "\n",
        "etc.\n",
        "\n",
        "This means that in a sense the total model input and model output are the same things. Hugging Face expects your training data to look like this:\n",
        "\n",
        "|  | padding | prefix part | output part | EOS indicator |\n",
        "|----------|----------|----------|----------|----------|\n",
        "| **input**   | tokenizer.pad_token_id | prefix token ids    | output token ids  | `tokenizer.pad_token_id`  |\n",
        "| **label (=output)**    |\\[-100,...,-100\\] |  \\[-100\\]*len(prefix)  |  output token ids   | `tokenizer.pad_token_id`   |\n",
        "| **attention mask**    |\\[0,...,0\\]| \\[1,...,1\\]   |  \\[1,...,1\\]   |  1   |\n",
        "\n",
        "The `-100` section means that the model doesn't need to learn the generation of this.\n",
        "\n",
        "The next thing you need to keep in mind is that text length vary inside a batch. To account for it, you need to pad the sequences (including the attention mask) with zeros (`tokenizer.pad_token_id`) **on the left** to some `max_length`. It can be overall max length or max length of the inputs and labels inside a batch.\n",
        "\n",
        "\n",
        "More specifically, HuggingFace transformer model will require a dictionary\n",
        "```\n",
        "{\n",
        "    \"input_ids\" : ..., # List of lists with input token id's\n",
        "    \"attention_mask\": ..., # List of 1 or 0, depending on whether a model should attend to the token. 0 is usually set for padding tokens.\n",
        "    \"labels\": ..., List of lists of id's of tokens we want our model to predict\n",
        "}\n",
        "```\n",
        "\n",
        "Each of the dictionary values should a `torch.tensor(v, dtype=int).to(device)` of shape `[batch_size, max_length]`, where `v` is list of lists."
      ],
      "metadata": {
        "id": "wzICSqWQY0Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batch(samples_batch, prefix, tokenizer, device='cuda'):\n",
        "    pass"
      ],
      "metadata": {
        "id": "6xCcQYO4cUuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now everything should be ready, let's fine tune just like we did in the practice session."
      ],
      "metadata": {
        "id": "2LN2M4H-cea1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        ")"
      ],
      "metadata": {
        "id": "kLh7aRUbV9Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LoRA config you also need to specify names of modules to insert LoRA adapters to. You can see all modules in `model.named_modules()`. Typically you want to include all Linear layers. In most cases you can either leave it blank or type \"all-linear\"."
      ],
      "metadata": {
        "id": "_Ns8EOo0VQhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=32,\n",
        "    target_modules='all-linear'\n",
        ")"
      ],
      "metadata": {
        "id": "UDhmT_veWYeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "lr = 1e-5\n",
        "num_epochs = 2\n",
        "batch_size = 4"
      ],
      "metadata": {
        "id": "yW3pVYMxWaxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "FbTBfwRCbiAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
        ")"
      ],
      "metadata": {
        "id": "hZssd-0cWjNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep in mind, we're finetuning quite a big model. It's only a couple epochs on a small dataset, but on a T4 this will still take around an hour."
      ],
      "metadata": {
        "id": "Qd90DtRmPDbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "PREFIX=\"Here's another elementary school math problem: \"\n",
        "\n",
        "model = model.cuda()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        model_inputs = preprocess_batch(batch, prefix=PREFIX, tokenizer=tokenizer)\n",
        "        outputs = model(**model_inputs)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.detach().float()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    eval_preds = []\n",
        "    for step, batch in enumerate(tqdm(test_dataloader)):\n",
        "        model_inputs = preprocess_batch(batch, prefix=PREFIX, tokenizer=tokenizer)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**model_inputs)\n",
        "        loss = outputs.loss\n",
        "        eval_loss += loss.detach().float()\n",
        "        eval_preds.extend(\n",
        "            tokenizer.batch_decode(\n",
        "                torch.argmax(outputs.logits, -1).detach().cpu().numpy(),\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "        )\n",
        "\n",
        "    eval_epoch_loss = eval_loss / len(test_dataloader)\n",
        "    eval_ppl = torch.exp(eval_epoch_loss)\n",
        "    train_epoch_loss = total_loss / len(train_dataloader)\n",
        "    train_ppl = torch.exp(train_epoch_loss)\n",
        "    print(f\"{epoch=}:\\n{train_ppl=}\\n{train_epoch_loss=}\\n{eval_ppl=}\\n{eval_epoch_loss=}\")"
      ],
      "metadata": {
        "id": "861RORjHWn5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally let's test it"
      ],
      "metadata": {
        "id": "wiZuRNg_ci1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.cuda()\n",
        "\n",
        "prefixes = [\n",
        "    'Here is another elementary school arithmetic problem: The elves',\n",
        "    'Here is another elementary school arithmetic problem: I thought that',\n",
        "    'Here is another elementary school arithmetic problem: Beavers',\n",
        "    'Here is another elementary school arithmetic problem: Generative AI',\n",
        "    'Here is another elementary school arithmetic problem: Billie had'\n",
        "]\n",
        "predictions = generate_math_problem(prefixes, model)\n",
        "for prediction in predictions:\n",
        "    display(prediction)\n",
        "\n",
        "model = model.cpu()"
      ],
      "metadata": {
        "id": "5INX5oBfXGq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And don't forget to save the result of your hard work :)"
      ],
      "metadata": {
        "id": "n-PtRFIXcq9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"math_finetune\")"
      ],
      "metadata": {
        "id": "IuCfxmJvcLrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus task 4.5\n",
        "*2 bonus points*\n",
        "\n",
        "Analise how different models handle generating math problems out of the box. Use models of comparible size (for example <7B) for this comparison to be more fair. You can use [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) to select models.\n",
        "\n",
        "You can also use APIs like OpenAI's, Anthropic, Gemini and etc. Those are not directly comparible with any OpenSource model, but may give you a much better result out of the box.\n",
        "\n",
        "We'd suggest using the following models:\n",
        "- [OLMo-1B](https://huggingface.co/allenai/OLMo-1B )\n",
        "- [QWEN1.5-0.5B-Chat](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat) or [Qwen1.5-0.5B](https://huggingface.co/Qwen/Qwen1.5-0.5B)\n",
        "- [Phi-3-mini](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)\n",
        "\n",
        "After you obtained the results, we want you to formulate a hypothesis about why those certain models are doing better or worse in this task. Please try to be as grounded as possible in your hypotheses, meaning that there should be at least some supporting evidence, and not just a blind guess. Some areas we would advise you to focus on:\n",
        "- The way the model was trained (next token prediction, instruct tuning, chat tuning, etc.)\n",
        "- What data was used to train the model (if available)"
      ],
      "metadata": {
        "id": "gPTksMnJ35n4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5\n",
        "In this task we'll take a look at [LLaVA](https://llava-vl.github.io/), a multimodal model combining LLM and visual encoder capabilities.\n",
        "\n",
        "First let's take a look at how you can inference LLaVA\n",
        "\n"
      ],
      "metadata": {
        "id": "B2Evqtxu2YlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.36.0\n",
        "!pip install -q bitsandbytes==0.41.3 accelerate==0.25."
      ],
      "metadata": {
        "id": "Xd7dOcKx28Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load an image we'll use a library called PIL. It's a standard library to handle images in Python"
      ],
      "metadata": {
        "id": "S2_-4kILP-2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "image_url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
        "image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "CNCjyy_JPm4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we're limited in resouces running on Collab, we'll use 4-bit quantization to run the model. In order to do that we'll use quantization config fron BitsAndBytes"
      ],
      "metadata": {
        "id": "V9J2dq2aQU6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "id": "hqpPqDKuQUTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will leverage the `image-to-text` pipeline from transformers !"
      ],
      "metadata": {
        "id": "nrZ_7U1oTQ3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})"
      ],
      "metadata": {
        "id": "DFVZgElEQk3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLaVA expects prompt in the following formats:\n",
        "```bash\n",
        "USER: <image>\\n<prompt>\\nASSISTANT:\n",
        "```"
      ],
      "metadata": {
        "id": "JvvtplWDRvfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 200\n",
        "prompt = \"USER: <image>\\nIf you were a painter, how would you call this image?\\nASSISTANT:\"\n",
        "\n",
        "outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})"
      ],
      "metadata": {
        "id": "W48r3NxDRskb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "XX80v0pgSr_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del pipe"
      ],
      "metadata": {
        "id": "_pdaDwyimjH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also reproduce this pipeline step-by-step:"
      ],
      "metadata": {
        "id": "VjPNWofJlHsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "model = LlavaForConditionalGeneration.from_pretrained(model_id, quantization_config=quantization_config)\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "c1dzTvotlHXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A \"processor\" includes both a tokenzer for text data and image processor for image data"
      ],
      "metadata": {
        "id": "FBoBGcb-nU--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.tokenizer)\n",
        "print(processor.image_processor)"
      ],
      "metadata": {
        "id": "uyEb8-u9nH32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we get the representation of an image:"
      ],
      "metadata": {
        "id": "j8fEEKEXoO6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values = processor.image_processor(image, return_tensors=\"pt\")['pixel_values']\n",
        "print(pixel_values.shape)\n",
        "model.config.vision_feature_layer # this is a variable, regulating which layer we take from a pretrained encoder.\n",
        "image_outputs = model.vision_tower(pixel_values, output_hidden_states=True)\n",
        "print(image_outputs[model.config.vision_feature_layer].shape)"
      ],
      "metadata": {
        "id": "-lAmB-DUoUTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to actually be able to put the image into a language model, LLaVA has a special projection layer"
      ],
      "metadata": {
        "id": "rxoxuHRCoumQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "projected_image = model.multi_modal_projector(image_outputs[model.config.vision_feature_layer])\n",
        "print(projected_image.shape)"
      ],
      "metadata": {
        "id": "rI51Grr3o2R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that now the image is in the same space as out tokens"
      ],
      "metadata": {
        "id": "8VfXi85rwpXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_input_embeddings()"
      ],
      "metadata": {
        "id": "vycjfdBNw0V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5.1\n",
        "\n",
        "*2 points*\n",
        "\n",
        "Use similar technique as in Task 1 to do the following:\n",
        "\n",
        "Take the projections of a couple images into the embedding space and find closest real tokens to those images.\n",
        "\n",
        "See if you can find any sort of patterns depending on the image you pass.\n",
        "\n",
        "\n",
        "Here's an illustration of what you need to extract:\n",
        "\n",
        "![Llava](https://drive.google.com/uc?id=1mUU2Lf8puAJNYKlCzYyF0MWiqmF-P_FU&export=download)"
      ],
      "metadata": {
        "id": "gjoDQ7nAvSgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5.2\n",
        "\n",
        "*2 points*\n",
        "\n",
        "Now that we know how to use this, let's try to create something fun from it.\n",
        "\n",
        "Create a function `flag_guesser`, which does the following:\n",
        "- It inputs a link to an image of a flag and a name of a country;\n",
        "- As an output it tells the user if they guessed the country correctly.\n",
        "\n",
        "If the image is not a flag, our function should not try to guess, but rather tell the user, that it's not a flag.\n",
        "\n",
        "Make sure that your function supports different image formats.\n",
        "\n",
        "Test your function on a couple of image and country combinations.\n",
        "\n",
        "Here's a small example:"
      ],
      "metadata": {
        "id": "nywEGeRC4db7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flag_guesser(image_url: str, country_name: str):\n",
        "    pass"
      ],
      "metadata": {
        "id": "7-NnpDWz2-ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/510px-Flag_of_the_Netherlands.svg.png\"\n",
        "Image.open(requests.get(image_url, stream=True).raw)"
      ],
      "metadata": {
        "id": "c3Ca_IRN6XP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "flag_guesser(image_url, \"France\")\n",
        "> No, that's not a flag of France\n",
        "flag_guesser(image_url, \"Netherlands\")\n",
        "> Yes, that's correct!\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UqYWVKYL6kEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag_guesser(image_url, \"France\")"
      ],
      "metadata": {
        "id": "TcMMxOqs7bTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag_guesser(image_url, \"Netherlands\")"
      ],
      "metadata": {
        "id": "uZ7O4DB87gJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kitten_url = \"https://icatcare.org/app/uploads/2018/07/Helping-your-new-cat-or-kitten-settle-in-1.png\"\n",
        "Image.open(requests.get(kitten_url, stream=True).raw)"
      ],
      "metadata": {
        "id": "vCuHK0WQ9aKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag_guesser(kitten_url, \"USA\")"
      ],
      "metadata": {
        "id": "xAgGc9Pr7iMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8ympifU9YFp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
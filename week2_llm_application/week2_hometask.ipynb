{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexUmnov/genai_course/blob/main/week2_llm_application/homework_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1. Question answering"
      ],
      "metadata": {
        "id": "cis-q9o4ivml"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UDXWTCS18d_"
      },
      "source": [
        "In this task you will practice using LangChain for question answering task.\n",
        "\n",
        "We will work with the dataset from the [Measuring Massive Multitask Language Understanding](https://arxiv.org/pdf/2009.03300) paper by Hendryks et al. It contains questions from fields as diverse as International Law, Nutrition and Higher Algebra. For each of the questions, 4 answers are given (labeled A-D) and one of them is marked as correct. We'll go for High School Mathematics.\n",
        "\n",
        "You can download the dataset from here https://people.eecs.berkeley.edu/~hendrycks/data.tar, then unzip uzing your system's dialogue (you can use 7-zip for example). However, we suggest downloading the data with help of Hugging Face [Dataset](https://huggingface.co/docs/datasets/index) library."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai tqdm openai datasets --quiet"
      ],
      "metadata": {
        "id": "jV2ps7T_4_DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cais/mmlu\", \"high_school_mathematics\", split=\"test\")"
      ],
      "metadata": {
        "id": "9KxGmdB2A1co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the dataset. What does it have for us?"
      ],
      "metadata": {
        "id": "caz7ePk1rnfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-Wlt9fq18eD"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save time and API calls costs we suggest evaluating only 50 examples from the dataset."
      ],
      "metadata": {
        "id": "i5Z-V9Cljz8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset[:50]"
      ],
      "metadata": {
        "id": "WOp79JFqkah6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YApEUjkf18eD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dataset = pd.DataFrame(dataset)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the answers are not labeled by letters A-D, so we'll do it manually."
      ],
      "metadata": {
        "id": "cxcS8Tr9DF46"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hphL2uD918eC"
      },
      "outputs": [],
      "source": [
        "questions = dataset[\"question\"]\n",
        "choices = pd.DataFrame(\n",
        "    data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "    )\n",
        "answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUmx0al018eE"
      },
      "source": [
        "Let's use Generative AI to predict the correct answer. We suggest using GPT-4o-mini, because it's both cheap and proficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYnCiE1q18eE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "open_ai_api_key = open(\".open-ai-api-key\").read().strip()\n",
        "# open_ai_api_key = userdata.get(\"open_ai_api_key\")\n",
        "os.environ['OPENAI_API_KEY'] = open_ai_api_key\n",
        "\n",
        "\n",
        "example_id = 0\n",
        "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=open_ai_api_key)\n",
        "result = chat.invoke([\n",
        "    HumanMessage(\n",
        "        content=f\"{questions[example_id]} \" \\\n",
        "        f\"A) {choices['A'][example_id]} \" \\\n",
        "        f\"B) {choices['B'][example_id]} \" \\\n",
        "        f\"C) {choices['C'][example_id]} \" \\\n",
        "        f\"D) {choices['D'][example_id]}\"\n",
        "        )\n",
        "])\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can observe that ChatGPT uses *chain-of-thought reasoning* to tackle this problem (see [Wei et al.](https://arxiv.org/pdf/2201.11903.pdf)). This is generally very helpful to approach math problems.\n",
        "\n",
        "**Note**. Even if the model avoids chain-of-thought reasoning, you can persuade it with prompts like: `\"Break down the question in multiple steps, write them down and then give the answer'\"`.\n",
        "\n",
        "But the thing is that we only need an answer. So, we need a way to extract the right letter from this lengty response."
      ],
      "metadata": {
        "id": "2d9PzBnQsVgm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHJTEL6D18eF"
      },
      "source": [
        "## Task 1.1. Zero-shot use\n",
        "\n",
        "Let's start by trying to supress chain-of-thought reasoning. We will ask the LLM to output just one letter A-D.\n",
        "\n",
        "Write a function doing it. Your solution should only rely on well chosen prompts, without any post-parsing of the output. Please always ensure during this task that you're using the \"gpt-4o-mini\" model. Otherwise your comparison will not be correct, and/or you accuracy/wrong format number may turn out to be worse than you would expect.\n",
        "\n",
        "**Hint 1**. You can use `SystemMessage` or just a well chosen prompt template. If you use `SystemMessage`, ensure that you are using a chat model.\n",
        "\n",
        "**Hint 2**. Don't forget to set temperature to zero. We need truthfulness, not creativity. Note however than even setting temperature to zero doesn't necessary mean that the completions will be completely reproducible. See [this discussion](https://community.openai.com/t/a-question-on-determinism/8185/2) for some hints.\n",
        "\n",
        "**Hint 3**. Don't forget to look at the outputs. It may greatly help you to create better prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMEzwZAM18eG"
      },
      "outputs": [],
      "source": [
        "def chatgpt_answer(question: str, a: str, b: str, c: str, d: str) -> str:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also provide you with the accuracy calculating function. Which also allows you to debug your answers by passing `verbose=True`"
      ],
      "metadata": {
        "id": "9Dt9qsP-1U0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_answers(answers, model_answers, verbose=False):\n",
        "    wrong_format = 0\n",
        "    correct = 0\n",
        "    wrong_answers = []\n",
        "    for correct_answer, model_answer in zip(answers, model_answers):\n",
        "        if correct_answer == model_answer[0]:\n",
        "            correct += 1\n",
        "        else:\n",
        "            wrong_answers.append(f\"Expected answer: {correct_answer} given answer {model_answer}\")\n",
        "        if (model_answer[0] not in [\"A\", \"B\", \"C\", \"D\"]) or len(model_answer) > 1:\n",
        "            wrong_format += 1\n",
        "\n",
        "    result = {\n",
        "        \"accuracy\": correct / len(answers),\n",
        "        \"wrong_format\": wrong_format / len(answers),\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        result['wrong_answers'] = wrong_answers\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "dK-kt19M1I3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we calculate two things:\n",
        "\n",
        "- Accuracy rate. Note that an answer is considered as accurate whenever it starts from the correct letter.\n",
        "- Wrong format rate. It penalizes all the answers which are not 'A', 'B', 'C', or 'D'."
      ],
      "metadata": {
        "id": "Xzv0Hz2mugPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdtXFgmg18eG"
      },
      "outputs": [],
      "source": [
        "chatgpt_answer(\n",
        "    questions[0],\n",
        "    choices.A[0],\n",
        "    choices.B[0],\n",
        "    choices.C[0],\n",
        "    choices.D[0],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MsfnLPW18eH"
      },
      "source": [
        "You may also experiment with other subjects, not only school math. The dataset has other subjects, you can see all of them [here](https://huggingface.co/datasets/cais/mmlu). You can pick the subject you like the most and evaluate your functions on it. However, you will need to submit your school math results for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raJPKiOz18eH"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpzQj5RC18eH"
      },
      "outputs": [],
      "source": [
        "model_answers = []\n",
        "for example_id in tqdm(range(len(dataset))):\n",
        "    model_answers.append(chatgpt_answer(\n",
        "        questions[example_id],\n",
        "        choices.A[example_id],\n",
        "        choices.B[example_id],\n",
        "        choices.C[example_id],\n",
        "        choices.D[example_id]\n",
        "    ))\n",
        "\n",
        "check_answers(answers, model_answers, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9wYjcHp18eH"
      },
      "source": [
        "Depending on the subject the accuracy may vary but generally it can be rather poor. It seems that getting rid of chain-of-though wasn't a good idea.\n",
        "\n",
        "*You should aim at getting at least 20% of the answers in correct format.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To submit**. You will need to submit a csv file with model answers. Please launch the code below and submit the `zero_shot_answers.csv` file to grading."
      ],
      "metadata": {
        "id": "AihhFo4wxAQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(model_answers, columns=['answer'])\n",
        "df.to_csv('zero_shot_answers.csv', index=True)"
      ],
      "metadata": {
        "id": "_SYrz-tZO4u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2. Ensuring format with few-shot examples\n",
        "\n",
        "In the previous task we tried to make the LLM obey a particular format by explaining this format. This time, we will do it by showing the LLM how it's done with few-shot examples.\n",
        "\n",
        "**Note:** You can implement Few-Shot in two ways:\n",
        "\n",
        "1. You can combine `SystemMessage`, `HumanMessage` and `AIMessage` to create a fake chat history, like this:\n",
        "\n",
        "```{python}\n",
        "chat.invoke([\n",
        "    SystemMessage(content=\"\"\"Answer every request only with formulas, using no single word\"\"\"),\n",
        "    HumanMessage(content=\"\"\"You gave me 3 apples and then took 2 apples from me. How many apples do I have now?\"\"\"),\n",
        "    AIMessage(content=\"\"\"3 - 2 = 1\"\"\")\n",
        "    HumanMessage(content=\"\"\"How can I convert Celsius to Fahrenheit?\"\"\"),\n",
        "    AIMessage(content=\"\"\"F=5/9*​C+32\"\"\")\n",
        "])\n",
        "\n",
        "```\n",
        "\n",
        "The next `HumanMessage` will be the user's real request.\n",
        "\n",
        "2. You can just write all the chat history in a single user message:\n",
        "\n",
        "```{python}\n",
        "chat.invoke([\n",
        "    HumanMessage(content=\"\"\"Answer every request only with formulas, using no single word.\n",
        "    \n",
        "    User: You gave me 3 apples and then took 2 apples from me. How many apples do I have now?\n",
        "    \n",
        "    Assistant: 3 - 2 = 1\n",
        "\n",
        "    ###\n",
        "\n",
        "    User: How can I convert Celsius to Fahrenheit?\n",
        "\n",
        "    Assistant: F=5/9*​C+32\n",
        "\n",
        "    ###\n",
        "\n",
        "    User: {real user's message}\n",
        "\n",
        "    Assistant:\n",
        "])\n",
        "```\n",
        "\n",
        "You will need to use Few Shot examples in the math Q&A task to ensure that GPT-4o-Mini outputs only answer codes (A, B, C, or D). Likely, you will observe that showing the right format to an LLM may be more efficient than explaining it.\n",
        "\n",
        "Try to retain as much of your previous prompt as possible. This will help us to understand the significance of this particular change.\n",
        "\n",
        "Evaluate the same subject you used earlier with Few-Shot prompt and compare the results."
      ],
      "metadata": {
        "id": "qQN40XoVkyqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_few_shot_answer(question: str, a: str, b: str, c: str, d: str) -> str:\n",
        "    pass"
      ],
      "metadata": {
        "id": "DFd-5xnMlG-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_few_shot_answer(\n",
        "    questions[0],\n",
        "    choices.A[0],\n",
        "    choices.B[0],\n",
        "    choices.C[0],\n",
        "    choices.D[0],\n",
        ")"
      ],
      "metadata": {
        "id": "_-4yQIw5uZLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_answers = []\n",
        "for example_id in tqdm(range(len(dataset))):\n",
        "    model_answers.append(chatgpt_few_shot_answer(\n",
        "        questions[example_id],\n",
        "        choices.A[example_id],\n",
        "        choices.B[example_id],\n",
        "        choices.C[example_id],\n",
        "        choices.D[example_id]\n",
        "    ))\n",
        "\n",
        "check_answers(answers, model_answers, verbose=True)"
      ],
      "metadata": {
        "id": "r8g53JUTlsLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*You should aim at at least 50% answers in the correct format*"
      ],
      "metadata": {
        "id": "_lh0CyARy3Xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To submit**. You will need to submit a csv file with model answers. Please launch the code below and submit the `few_shot_answers.csv` file to grading."
      ],
      "metadata": {
        "id": "_6P_jJ_cR9_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(model_answers, columns=['answer'])\n",
        "df.to_csv('few_shot_answers.csv', index=True)"
      ],
      "metadata": {
        "id": "6VLPHphWR9_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.3. Chain-of-thoughts\n",
        "\n",
        "Okay, let's confess that, even though we were able to do a decent job with the format, without chain-of-thought reasoning the accuracy is not good. Now, let's allow the LLM to \"think out loud\" and then use it again to extract the answer from the chain-of-though output (as one letter).\n",
        "\n",
        "Implement these two LLM calls in one function.\n",
        "\n",
        "**Note:** Don't forget to feed the answer of the first LLM to the second LLM.\n",
        "**Note:** If your prompt gets too long due to few shot examples, it's usually a good idea to repeat the question in the end. A model might \"forget\" what the question was.\n",
        "\n",
        "Try to retain as much of your previous prompt as possible. This will help us to understand the significance of this particular change."
      ],
      "metadata": {
        "id": "vVTSf2zHzdlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_step_by_step_answer(question: str, a: str, b: str, c: str, d: str) -> str:\n",
        "    chat = ChatOpenAI(temperature=0)\n",
        "    messages = ...\n",
        "    step_by_step_response = ...\n",
        "    messages.append(AIMessage(content=step_by_step_response))\n",
        "    parsed_response = ..."
      ],
      "metadata": {
        "id": "zlIHmc7Sz2px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**. This function is not a LangChain chain, just a chat. But in a sence a chat works like a chain. The main difference is that proper chains are better structured:\n",
        "\n",
        "- In a proper chain we construct prompt templates to facilitate putting together different inputs and outputs. We can instruct an LLM about the relations between them.\n",
        "- In a chat we have all the inputs and outputs piled together as messages, and we rely on ability of an LLM to extract information from discussions.\n",
        "\n",
        "**Note**. If you don't get the desired quality, it's a good idea to look at the both reasoning and the answers. This will help you to debug all the process. So, we advise you to keep all the outputs."
      ],
      "metadata": {
        "id": "pZE5qL7ZfzZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_step_by_step_answer(\n",
        "    questions[0],\n",
        "    choices.A[0],\n",
        "    choices.B[0],\n",
        "    choices.C[0],\n",
        "    choices.D[0],\n",
        ")"
      ],
      "metadata": {
        "id": "vLR5gtGC1r4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_answers = []\n",
        "for example_id in tqdm(range(len(dataset))):\n",
        "    model_answers.append(chatgpt_step_by_step_answer(\n",
        "        questions[example_id],\n",
        "        choices.A[example_id],\n",
        "        choices.B[example_id],\n",
        "        choices.C[example_id],\n",
        "        choices.D[example_id]\n",
        "    ))\n",
        "\n",
        "check_answers(answers, model_answers, verbose=True)\n"
      ],
      "metadata": {
        "id": "1QUJDkG23HBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should aim at getting at least 60% of your answers in the correct format and accuracy at least 40%"
      ],
      "metadata": {
        "id": "L12owB3xzZiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To submit**. You will need to submit a csv file with model answers. Please launch the code below and submit the `cot_answers.csv` file to grading."
      ],
      "metadata": {
        "id": "vsUoFXlFZT7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(model_answers, columns=['answer'])\n",
        "df.to_csv('cot_answers.csv', index=True)"
      ],
      "metadata": {
        "id": "c87uRCBpZT7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus task 1.4*\n",
        "\n",
        "Rewrite `chatgpt_step_by_step_answer` with chains. Compare the quality."
      ],
      "metadata": {
        "id": "yx-68H418HPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_step_by_step_answer(\n",
        "    questions[0],\n",
        "    choices.A[0],\n",
        "    choices.B[0],\n",
        "    choices.C[0],\n",
        "    choices.D[0],\n",
        ")"
      ],
      "metadata": {
        "id": "LWUUpWxo9RNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.5. Self-consistency: an ensemble of Chains of Thoughts\n",
        "\n",
        "There's another popular and effective method of getting a better answer from your LLM.\n",
        "\n",
        "We already know that adding a chain-of-thought answer improves the quality, because the model has some \"space\" to \"think\" about the answer.\n",
        "But we can make it even better by allowing the model to generate multiple chains of thought to obtain candidates for the answer and then choose the best of those.\n",
        "\n",
        "This is essentially the method introduced by this paper [Self-Consistency Improves Chain of Thought Reasoning in Language Models\n",
        "](https://arxiv.org/abs/2203.11171).\n",
        "\n",
        "In practice you need to make a function `chatgpt_self_consistency_answer` , which will do the following:\n",
        "- Generate a diverse set of answers (for this it's good to set the [sampling temperature](https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature) greater than 0). You might need to try a bunch of different value to get a better result;\n",
        "- Exctract final answer from those step-by-step explanations;\n",
        "- Select the best answer based on majority vote.\n",
        "\n",
        "To make a fair comparison try to retain as much of your previous step-by-step prompt as possible.\n",
        "\n",
        "However, you might want to do a bit more to ensure that your answers are in the same (right) format, because otherwise the majority vote doesn't make a ton of sense.\n",
        "\n",
        "**Note:** If you were to run it on a full dataset or a big part of it, be aware, that this takes much longer, because it's essintialy num_runs times more calls.\n",
        "\n",
        "**Bonus:** If you want to make your code faster and more true to how it would be handled in a real usecase, take a look at [asyncio](https://docs.python.org/3/library/asyncio.html). You can launch different calls asynchronously.\n",
        "\n",
        "**Bonus:** We also encourage you to implement your function with LangChain chains."
      ],
      "metadata": {
        "id": "ZC1MOZ60deE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_self_consistency_answer(\n",
        "        question: str,\n",
        "        a: str,\n",
        "        b: str,\n",
        "        c: str,\n",
        "        d: str,\n",
        "        num_runs: int = 5,\n",
        "        sampling_temperature=0.7\n",
        "    ) -> str:\n",
        "    answers = []\n",
        "    for _ in range(num_runs):\n",
        "        '''Run several Chains of Thoughts + answer extraction'''\n",
        "\n",
        "    '''Choose the most popular answer and return it'''"
      ],
      "metadata": {
        "id": "kwb72u7y_Pao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_self_consistency_answer(\n",
        "    questions[0],\n",
        "    choices.A[0],\n",
        "    choices.B[0],\n",
        "    choices.C[0],\n",
        "    choices.D[0],\n",
        ")"
      ],
      "metadata": {
        "id": "w6PDWTRElEga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_answers = []\n",
        "for example_id in tqdm(range(len(dataset))):\n",
        "    model_answers.append(chatgpt_self_consistency_answer(\n",
        "        questions[example_id],\n",
        "        choices.A[example_id],\n",
        "        choices.B[example_id],\n",
        "        choices.C[example_id],\n",
        "        choices.D[example_id]\n",
        "    ))\n",
        "\n",
        "check_answers(answers, model_answers, verbose=True)\n"
      ],
      "metadata": {
        "id": "U7o9oo3Mlq00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To submit**. You will need to submit a csv file with model answers. Please launch the code below and submit the `self_consistency_answers.csv` file to grading."
      ],
      "metadata": {
        "id": "ASTCDUnhaY4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(model_answers, columns=['answer'])\n",
        "df.to_csv('self_consistency_answers.csv', index=True)"
      ],
      "metadata": {
        "id": "ySaE4j3caY4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should aim at getting at least 75% of your answers in the correct format and accuracy at least 50%"
      ],
      "metadata": {
        "id": "z0pE-xS-bSki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.6 Structured Output\n",
        "\n",
        "Finally we'll try to focus on format more than quality. Even though with previous techniques you might already be getting quite good results for `wrong_answer` metric, it's still important to exercise using **structured output**. It's consistent and much more appropriate to real world application, rather than toy problems.\n",
        "\n",
        "You task is to finish the following function (use whichever prompt you like from previous sub-tasks). You need to design the structure yourself in a way, which you think is more appropriate for this task.\n",
        "\n",
        "For some inspiration we advise you to take a look at supported schemas https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n",
        "\n",
        "To submit the solution, convert your predicted models to the same format as before.\n",
        "\n",
        "Note: To use structured output with langchain, you can use the [following](https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output):\n",
        "\n",
        "```\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "from langchain_openai import ChatOpenAI=\n",
        "\n",
        "\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "structured_llm = model.with_structured_output(Joke)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joke about cats\")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "fVYc985k-m1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "def chatgpt_structured_answer(\n",
        "        question: str,\n",
        "        a: str,\n",
        "        b: str,\n",
        "        c: str,\n",
        "        d: str,\n",
        "        output_model: BaseModel\n",
        "    ) -> str:\n",
        "        pass"
      ],
      "metadata": {
        "id": "5zS2RN1w_cV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_answers = []\n",
        "for example_id in tqdm(range(len(dataset))):\n",
        "    model_answers.append(chatgpt_structured_answer(\n",
        "        questions[example_id],\n",
        "        choices.A[example_id],\n",
        "        choices.B[example_id],\n",
        "        choices.C[example_id],\n",
        "        choices.D[example_id]\n",
        "    ))\n",
        "\n",
        "check_answers(answers, model_answers, verbose=True"
      ],
      "metadata": {
        "id": "CFgRdyCEhMWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To submit**"
      ],
      "metadata": {
        "id": "6tAM3ecVhHOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(model_answers, columns=['answer'])\n",
        "df.to_csv('structured_answer.csv', index=True"
      ],
      "metadata": {
        "id": "423GffH4hGn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_structured_answer(\n",
        "    questions[0],\n",
        "    choices.A[0],\n",
        "    choices.B[0],\n",
        "    choices.C[0],\n",
        "    choices.D[0],\n",
        "    MMLUAnswerModel\n",
        ")"
      ],
      "metadata": {
        "id": "k-A-H90TBT4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_answers = []\n",
        "for example_id in tqdm(range(len(dataset))):\n",
        "    model_answers.append(chatgpt_structured_answer(\n",
        "        questions[example_id],\n",
        "        choices.A[example_id],\n",
        "        choices.B[example_id],\n",
        "        choices.C[example_id],\n",
        "        choices.D[example_id],\n",
        "        MMLUAnswerModel\n",
        "    ))\n",
        "\n",
        "check_answers(answers, model_answers, verbose=True)"
      ],
      "metadata": {
        "id": "gB1VG2gkCHgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2. Introducing vector database search\n",
        "\n",
        "*3 points*\n",
        "\n",
        "In the previous task we solved Q&A task with an LLM using only whatever LLM has \"learnt\" during its training. However, this doesn't always work perfectly. Often, you just need to indroduce specific knowledge to the LLM to get adequate quality of generation. This is usually done by allowing an LLM to search for answers in the net or in some database.\n",
        "\n",
        "In this task you'll learn to query vector databases with LLMs. We will mainly follow a tutorial of `lancedb`.\n",
        "\n",
        "Let's install prerequisites."
      ],
      "metadata": {
        "id": "5P4mFvpaR_wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lancedb datasets tqdm openai langchain langchain_community -q"
      ],
      "metadata": {
        "id": "HcBGF1CrSs04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import openai\n",
        "\n",
        "from langchain.vectorstores import LanceDB\n",
        "from langchain.schema import Document\n",
        "\n",
        "import lancedb\n",
        "from lancedb.embeddings import with_embeddings"
      ],
      "metadata": {
        "id": "QR87WWlhlplW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the experiments we'll use `truthful_qa` dataset, which provides both popular misconsceptions and correct answers to a number of question. This dataset is used in research to test generative AI's *truthfullness*."
      ],
      "metadata": {
        "id": "Zew3Js8yTCPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"truthful_qa\", \"generation\", split='validation')\n",
        "dataset"
      ],
      "metadata": {
        "id": "_8CTde2LSvxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "ZShQBp5eTBo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to search by questions."
      ],
      "metadata": {
        "id": "3pfk9v-DO91D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = dataset.to_pandas()\n",
        "dataset_df['text'] = dataset_df['question']"
      ],
      "metadata": {
        "id": "zoaG9VsOO7XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our database."
      ],
      "metadata": {
        "id": "KsrbgzCJFmY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This line is needed in case you've ran this cell before to clear the db dir\n",
        "!rm -rf /tmp/lancedb\n",
        "\n",
        "db = lancedb.connect(\"/tmp/lancedb\")"
      ],
      "metadata": {
        "id": "k5BYYI-bFpU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can choose our embeddings and populate LanceDB tables."
      ],
      "metadata": {
        "id": "k6UuY4qTF_1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lancedb.embeddings import with_embeddings\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# open_ai_key = open(\".open-ai-api-key\").read().strip()\n",
        "open_ai_key = userdata.get('open_ai_api_key')\n",
        "openai.api_key = open_ai_key\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = open_ai_key\n",
        "\n",
        "def embed_func(c):\n",
        "    rs = openai.embeddings.create(input=c, model=\"text-embedding-ada-002\")\n",
        "    return [record.embedding for record in rs.data]\n",
        "\n",
        "data = with_embeddings(embed_func, dataset_df, show_progress=True)"
      ],
      "metadata": {
        "id": "khZodJWfNvgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truthful_qa_table = db.create_table('truthful_qa', data=data)"
      ],
      "metadata": {
        "id": "KI7GUkwjPdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_table(query, limit=5, table=truthful_qa_table):\n",
        "    query_embedding = embed_func(query)[0]\n",
        "    return table.search(query_embedding).limit(limit).to_pandas()\n",
        "\n",
        "def create_prompt(query, context):\n",
        "    return f\"Using this information: {context}\\n\\n\\n{query}\""
      ],
      "metadata": {
        "id": "CPCmaEpgPs6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function `search_result_to_context` which takes an output from db and returns textual context, which we'll feed to our LLM."
      ],
      "metadata": {
        "id": "wddNgXVqbn6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_result_to_context(search_result) -> str:\n",
        "    pass"
      ],
      "metadata": {
        "id": "f1qB7TlQb1nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = search_table(\"pigs\", limit=2)\n",
        "print(search_result_to_context(result))"
      ],
      "metadata": {
        "id": "ZAVedXzOR3Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now write two functions:\n",
        "\n",
        "- one providing ChatGPT's answer given a query, but without accessing our database;\n",
        "- another which also uses the database to incorporate the context.\n",
        "\n",
        "Make sure that the second function accepts `prompt_func`, a function, which creates a contextualised prompt."
      ],
      "metadata": {
        "id": "VDXYptp3cH0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def raw_answer(query) -> str:\n",
        "    pass\n",
        "\n",
        "def answer_with_db(query, prompt_func=create_prompt) -> str:\n",
        "    pass"
      ],
      "metadata": {
        "id": "KgxZ2jGccYJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "prompt = \"Can pigs fly?\"\n",
        "\n",
        "print(\"Raw answer\")\n",
        "display(raw_answer(prompt))\n",
        "\n",
        "print(\"\\n\\nAnswer using the database\")\n",
        "display(answer_with_db(prompt))\n"
      ],
      "metadata": {
        "id": "vI57dE_qXV4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus task\n",
        "\n",
        "*1 point*\n",
        "\n",
        "Now you need to write two new `prompt_func`. They should achieve the following goals:\n",
        "\n",
        "\n",
        "1.   Only give false information answering users query. (Keep in mind that ChatGPT would be very reluctant to do so, so you should somehow persuade it)\n",
        "2.   For any answer the models gives, make it cite a source from the context received.\n",
        "\n"
      ],
      "metadata": {
        "id": "AZLMlCbNcj2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_false_information_prompt(query, context) -> str:\n",
        "    pass"
      ],
      "metadata": {
        "id": "skLZ414jc3VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(answer_with_db(prompt, prompt_func=create_false_information_prompt))"
      ],
      "metadata": {
        "id": "jdgCXEbaawF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_with_source_prompt(query, context) -> str:\n",
        "    pass"
      ],
      "metadata": {
        "id": "NBe-GlqAdAlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(answer_with_db(prompt, prompt_func=create_with_source_prompt))"
      ],
      "metadata": {
        "id": "ZMTnzuMGa-SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.2\n",
        "\n",
        "*1 point*\n",
        "\n",
        "In this task you will write your own plugin for ChatGPT.\n",
        "\n",
        "The `langchain` library has `Tool.from_function` method, which allows you to turn your `str->str` function into a tool for your LLM. You will need to make this function, `db_tool_function`.\n",
        "\n",
        "Based on the description of our tool, the LLM agent will generate a string, which will be passed to this funciton. The output string will be the result, which the agent will see and try to use in answering your query.\n",
        "\n",
        "In the end it should be used like this:\n",
        "\n",
        "```\n",
        "tools = [\n",
        "    Tool.from_function(\n",
        "        func=db_tool_function,\n",
        "        name=..., # a fitting name\n",
        "        description=... # a descriptions to help the agent use it\n",
        "    ),\n",
        "]\n",
        "agent = initialize_agent(\n",
        "    tools=tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")\n",
        "agent.run(\n",
        "    \"What are the common misconceptions about food? List them all\"\n",
        ")\n",
        "# Agent goes to search the database\n"
      ],
      "metadata": {
        "id": "AOCd6vLoASzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai langchainhub openai"
      ],
      "metadata": {
        "id": "bIUACklJKgsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def db_tool_function(query: str) -> str:\n",
        "    pass"
      ],
      "metadata": {
        "id": "WdXTsodQApeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.tools import  Tool, tool\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "tools = [\n",
        "    Tool.from_function(\n",
        "        func=db_tool_function,\n",
        "        name=\"Search misconseptions database\",\n",
        "        description=\"Useful when you are talking about misconceptions, gives you hard questions and correct answers\"\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "e5OQjxKiY9qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "llm = OpenAI()\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "\n",
        "agent = create_react_agent(llm, tools, prompt)"
      ],
      "metadata": {
        "id": "DsK7hefQZWLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "CYRt95S_Lz0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\n",
        "    \"input\": \"What are the common misconceptions about food? List them all\",\n",
        "})"
      ],
      "metadata": {
        "id": "Oa_U6BkdZcgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2.3\n",
        "\n",
        "Let's take a closer look at the output, which out database search returns:"
      ],
      "metadata": {
        "id": "-eOSMNnxLwXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_table(query=\"pigs fly\")"
      ],
      "metadata": {
        "id": "e0LHzvKvLiBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, one of the columns is actually `distance`. In this task, we suggest you to implement the following system:\n",
        "\n",
        "For each query we search the database. We check whether at least one of the answers is closer than 0.2 and if yes - we answer with gpt-4o-mini using the database information. Otherwise, the output is generated by gpt-4o.\n",
        "\n",
        "This will emulate a real scenario where \"harder\" queries are processed by a bigger \"stronger\" model."
      ],
      "metadata": {
        "id": "LbG_mYaaMA09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_with_smart_routing(query, simple_model='gpt-4o-mini', complex_model='gpt-4o') -> str:\n",
        "    pass"
      ],
      "metadata": {
        "id": "ny2iAZS_NW1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_with_smart_routing(\"Can pigs fly?\")"
      ],
      "metadata": {
        "id": "bLBrnZmSNzfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_with_smart_routing(\"What is the theorem of Pythagoras?\")"
      ],
      "metadata": {
        "id": "twaINBVFN2AV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}